tokenized_text="Jim Henson was a puppeteer".split()
print(tokenized_text)
"""
There are also variations of word tokenizers that have extra rules for punctuation. With this kind of tokenizer, we can end up with some pretty large “vocabularies,” where a vocabulary is defined by the total number of independent tokens that we have in our corpus.
"""